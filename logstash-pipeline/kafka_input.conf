input {
  kafka {
    codec => json
    bootstrap_servers => "kafka:9092"
    topics => ["request-processed-event"]
    group_id => "logstash-group12"
    auto_offset_reset => "earliest"
  }
}

filter {
  json {
    source => "message"
    target => "parsed"
    add_tag => [ "json_parsed" ]
  }

  ruby {
    code => "
      begin
        data = event.get('parsed')
        if data.is_a?(Hash)
          event.set('id', data.dig('data', 'id'))
          event.set('deviceId', data.dig('data', 'deviceId'))
 
          document = data.dig('data', 'document')
          if document && document.length >= 3
            masked_document = 'XXX.XXX.XXX-' + document[-3..-1]
            event.set('document', masked_document)
          else
            event.set('document', document)
          end

          event.set('ip', data.dig('data', 'ip'))
          event.set('platform', data.dig('data', 'platform'))
          event.set('lat', data.dig('data', 'lat'))
          event.set('long', data.dig('data', 'long'))
          event.set('createdAt', data['createdAt'])
        else
          event.tag('json_parse_error')
        end
      rescue => e
        event.tag('ruby_exception')
        event.set('exception_message', e.message)
      end
    "
  }

  mutate {
    remove_field => ["parsed", "message", "@version", "@timestamp", "event"]
  }
}

output {
  stdout {
    codec => rubydebug
  }
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "event-request-processed12"
    document_id => "%{id}"
  }
}
